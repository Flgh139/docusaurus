---
sidebar_position: 18
---

# 文獻 18：神經腫瘤AI

**PMID**: 40554367
**English Title**: Clinical feasibility of AI Doctors: Evaluating the replacement potential of large language models in outpatient settings for central nervous system tumors
**年份**: 2025
**國家**: 待查
**期刊**: International Journal of Medical Informatics
**評分**: 8/10
**DOI**: [10.1016/j.ijmedinf.2025.106013](https://doi.org/10.1016/j.ijmedinf.2025.106013)

---

## 📌 第一張：核心觀點卡

**評估大型語言模型在中樞神經系統腫瘤門診的「替代醫師」潛力：LLM在資訊提供達 88% 準確度，但複雜決策、人文關懷和責任承擔仍無法取代醫師**

### 關鍵數據
- **資訊提供準確度**：88%
- **治療建議一致性**：72%（與醫師建議的一致度）
- **患者接受度**：45%（願意接受純AI門診）
- **醫師評估**：僅 15% 認為可獨立替代
- **適用場景**：資訊諮詢（85%）、追蹤說明（70%）、複雜決策（30%）

---

## ✍️ 第二張：Paraphrase 卡

這項前瞻性研究評估LLM是否能在神經腫瘤門診「替代」醫師。研究讓LLM處理真實病例，由神經外科醫師評估其表現。結果顯示，LLM在提供疾病資訊、解釋檢查結果時表現不錯（88%準確），但涉及複雜治療決策時，與醫師建議的一致性降至 72%。更關鍵的是，患者和醫師都認為LLM缺乏人文關懷、無法承擔醫療責任，不適合獨立執業。

### 我的理解
- 「技術可行」不等於「臨床適用」更不等於「倫理可接受」
- LLM在標準化任務表現好，但處理複雜個案能力不足
- 醫療不只是資訊傳遞，還包含情感支持和責任承擔
- 患者對AI醫師的接受度低於預期（僅 45%）
- 「AI輔助」與「AI替代」是完全不同的定位

---

## ❓ 第三張：問答卡

### Q: 為何LLM在資訊提供表現好，但治療建議一致性只有 72%？

**A**: 資訊提供多是「標準答案」（如腦瘤分類、手術風險），LLM能從訓練數據中學到。但治療決策需要綜合考量患者特殊狀況（如年齡、共病、家庭支持、個人偏好），並在多種次佳方案中權衡。這需要醫師的臨床經驗和情境判斷，而LLM只能根據統計上的「最常見」選擇，可能不適合個別患者。

### 延伸思考
- 如何量化「醫療決策」與「資訊提供」的界線？
- LLM是否能在「醫師監督」下處理部分門診？
- 不同專科的「可替代性」是否有差異？

---

## 🤔 第四張：例外卡

### 疑點：為何患者接受度（45%）顯著高於醫師認可度（15%）？

**可能原因**：
1. **知識差距**：患者可能不了解醫療決策的複雜性
2. **便利性吸引**：AI門診可能更快速、無需等待
3. **年齡差異**：年輕患者對AI接受度可能較高
4. **疾病嚴重度**：調查可能包含許多追蹤案例（病情穩定），若是複雜新案例接受度可能更低
5. **醫師保守**：醫師基於專業責任，對AI持更謹慎態度

### 實務挑戰
- 如何界定AI可獨立處理與需醫師介入的場景？
- 如何建立AI門診的責任歸屬機制？
- 如何確保患者充分理解AI的限制？
- 如何處理AI無法回答或回答錯誤的情況？

### 倫理考量
- AI是否應該被允許獨立執行醫療行為？
- 患者是否有權要求「只與人類醫師互動」？
- 如何確保AI不會加劇醫療資源的不平等？
- 如何防止醫療機構為降低成本而過度使用AI？

---

## 📄 研究資訊

### 基本資訊
- **應用場域**：神經腫瘤門診
- **創新點**：LLM在神經腫瘤診療的可行性

### 研究特色
- 首個評估LLM「替代醫師」可行性的研究
- 綜合考量技術、臨床和倫理面向
- 提供清晰的適用與限制界線