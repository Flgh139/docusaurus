---
sidebar_position: 63
---

# æ–‡ç» 63ï¼šAssessing LLM-generated vs expert-created clinical anatomy MCQs

**English Title**: Assessing LLM-generated vs expert-created clinical anatomy MCQs

**ä¸­æ–‡æ¨™é¡Œ**: è©•ä¼° LLM ç”Ÿæˆèˆ‡å°ˆå®¶å‰µå»ºçš„è‡¨åºŠè§£å‰–å­¸é¸æ“‡é¡Œ
**PMID**: 40884796
**æœŸåˆŠ**: Medical education online
**è©•åˆ†**: 8.0
**æ‡‰ç”¨é ˜åŸŸ**: é†«å­¸æ•™è‚²
**DOI**: https://doi.org/10.1080/10872981.2025.2554678

---

## ğŸ“Œ ç¬¬ä¸€å¼µï¼šæ ¸å¿ƒè§€é»å¡

### ä¸»è¦ç™¼ç¾
AIç”ŸæˆMCQå“è³ªè©•ä¼°

### æ–‡ç»æ‘˜è¦ï¼ˆAbstractï¼‰
Large language models (LLMs) such as ChatGPT and Gemini are increasingly used to generate educational content in medical education, including multiple-choice questions (MCQs), but their effectiveness compared to expert-written questions remains underexplored, particularly in anatomy. We conducted a cross-sectional, mixed-methods study involving Year 2-4 medical students at Qatar University, where participants completed and evaluated three anonymized MCQ sets-authored by ChatGPT, Google-Gemini, and a clinical anatomist-across 17 quality criteria. Descriptive and chi-square analyses were performed, and optional feedback was reviewed thematically. Among 48 participants, most rated the three MCQ sources as equally effective, although ChatGPT was more often preferred for helping students identify and confront their knowledge gaps through challenging distractors and diagnostic insight, while expert-written questions were rated highest for deeper analytical thinking. A significant variation in preferences was observed across sources (Ï‡Â² (64) = 688.79,

### é—œéµæ•¸æ“š
- **ç ”ç©¶é¡å‹**: æœªåˆ†é¡
- **åœ‹å®¶/åœ°å€**: å¾…æŸ¥
- **ç™¼è¡¨å¹´ä»½**: å¾…æŸ¥
- **ROI æ•¸å€¼**: ç„¡æ•¸å€¼

### æ ¸å¿ƒå‰µæ–°é»
æœ¬ç ”ç©¶çš„ä¸»è¦å‰µæ–°åœ¨æ–¼ï¼šAIç”ŸæˆMCQå“è³ªè©•ä¼°

---

## âœï¸ ç¬¬äºŒå¼µï¼šParaphrase å¡

### ç ”ç©¶ç›®çš„èˆ‡æ–¹æ³•
æœ¬ç ”ç©¶èšç„¦æ–¼é†«å­¸æ•™è‚²é ˜åŸŸï¼Œæ—¨åœ¨æ¢è¨ AI æŠ€è¡“åœ¨é†«ç™‚å ´æ™¯ä¸­çš„æ‡‰ç”¨ã€‚

### ä¸»è¦è²¢ç»
ç ”ç©¶æå‡ºäº†AIç”ŸæˆMCQå“è³ªè©•ä¼°ï¼Œç‚ºè‡¨åºŠå¯¦è¸æä¾›äº†æ–°çš„è§£æ±ºæ–¹æ¡ˆã€‚

### æŠ€è¡“ç‰¹é»
- æ‡‰ç”¨å ´æ™¯ï¼šé†«å­¸æ•™è‚²
- å‰µæ–°ç¨‹åº¦ï¼šè©•åˆ† 8.0/10
- æœŸåˆŠå½±éŸ¿ï¼šç™¼è¡¨æ–¼ Medical education online

---

## â“ ç¬¬ä¸‰å¼µï¼šå•ç­”å¡

### Q1: é€™é …ç ”ç©¶è§£æ±ºäº†ä»€éº¼å•é¡Œï¼Ÿ
A: æœ¬ç ”ç©¶é‡å°é†«å­¸æ•™è‚²é ˜åŸŸçš„æŒ‘æˆ°ï¼Œæå‡ºäº†åŸºæ–¼ AI çš„è§£æ±ºæ–¹æ¡ˆã€‚

### Q2: æ¡ç”¨äº†ä»€éº¼æŠ€è¡“æ–¹æ³•ï¼Ÿ
A: AIç”ŸæˆMCQå“è³ªè©•ä¼°

### Q3: ç ”ç©¶çš„ä¸»è¦æˆæœæ˜¯ä»€éº¼ï¼Ÿ
A: ç ”ç©¶ç²å¾—äº† 8.0 åˆ†çš„å°ˆå®¶è©•åˆ†ï¼Œè­‰æ˜äº†å…¶åœ¨é†«å­¸æ•™è‚²é ˜åŸŸçš„æ‡‰ç”¨åƒ¹å€¼ã€‚

### Q4: æœ‰å“ªäº›å¯¦éš›æ‡‰ç”¨å ´æ™¯ï¼Ÿ
A: ä¸»è¦æ‡‰ç”¨æ–¼é†«å­¸æ•™è‚²ï¼Œå¯ç”¨æ–¼æ”¹å–„é†«ç™‚æœå‹™å“è³ªå’Œæ•ˆç‡ã€‚

---

## ğŸ¤” ç¬¬å››å¼µï¼šä¾‹å¤–å¡

### æ½›åœ¨é™åˆ¶
1. **ç ”ç©¶ç¯„åœ**: ç ”ç©¶è©•åˆ†ç‚º 8.0/10ï¼Œå¯èƒ½å­˜åœ¨æ”¹é€²ç©ºé–“
2. **åœ°å€å·®ç•°**: ç ”ç©¶ä¾†è‡ªå¾…æŸ¥ï¼Œå…¶ä»–åœ°å€é©ç”¨æ€§éœ€é©—è­‰
3. **ROI æ•¸æ“š**: ç„¡æ•¸å€¼ï¼Œç¶“æ¿Ÿæ•ˆç›Šè©•ä¼°å¯èƒ½ä¸å®Œæ•´

### éœ€è¦é€²ä¸€æ­¥æ¢è¨çš„å•é¡Œ
1. è©²æŠ€è¡“åœ¨ä¸åŒè¦æ¨¡é†«é™¢çš„é©ç”¨æ€§å¦‚ä½•ï¼Ÿ
2. é•·æœŸä½¿ç”¨çš„æ•ˆæœå’Œç©©å®šæ€§æ˜¯å¦ç¶“éé©—è­‰ï¼Ÿ
3. èˆ‡ç¾æœ‰ç³»çµ±æ•´åˆæ™‚å¯èƒ½é¢è‡¨å“ªäº›æŒ‘æˆ°ï¼Ÿ

### æ‰¹åˆ¤æ€§æ€è€ƒ
- **æŠ€è¡“æˆç†Ÿåº¦**: éœ€è¦è©•ä¼°å¾ç ”ç©¶åˆ°å¯¦éš›éƒ¨ç½²çš„è·é›¢
- **æˆæœ¬æ•ˆç›Š**: ç„¡æ•¸å€¼ï¼Œéœ€è¦æ›´è©³ç´°çš„ç¶“æ¿Ÿåˆ†æ
- **å€«ç†è€ƒé‡**: AI åœ¨é†«å­¸æ•™è‚²æ‡‰ç”¨æ™‚çš„å€«ç†å’Œéš±ç§å•é¡Œ
- **å¯æ¨å»£æ€§**: ç ”ç©¶çµæœåœ¨å…¶ä»–é†«ç™‚å ´æ™¯çš„é©ç”¨æ€§

---

## ğŸ“š åƒè€ƒè³‡è¨Š
- **PMID**: [40884796](https://pubmed.ncbi.nlm.nih.gov/40884796/)
- **DOI**: https://doi.org/10.1080/10872981.2025.2554678
- **æœŸåˆŠ**: Medical education online
