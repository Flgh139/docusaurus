---
sidebar_position: 72
---

# æ–‡ç» 72ï¼šAI-Driven Integrated System for Burn Depth Prediction With Electronic Medical Records

**English Title**: AI-Driven Integrated System for Burn Depth Prediction With Electronic Medical Records

**ä¸­æ–‡æ¨™é¡Œ**: åŸºæ–¼é›»å­ç—…æ­·çš„ AI é©…å‹•ç‡’å‚·æ·±åº¦é æ¸¬é›†æˆç³»çµ±
**PMID**: 40815778
**æœŸåˆŠ**: æœªçŸ¥æœŸåˆŠ
**è©•åˆ†**: é«˜
**æ‡‰ç”¨é ˜åŸŸ**: 9
**DOI**: https://doi.org/10.2196/68366

---

## ğŸ“Œ ç¬¬ä¸€å¼µï¼šæ ¸å¿ƒè§€é»å¡

### ä¸»è¦ç™¼ç¾
é«˜

### æ–‡ç»æ‘˜è¦ï¼ˆAbstractï¼‰
BACKGROUND: Burn injuries represent a significant clinical challenge due to the complexity of accurately assessing burn depth, which directly influences the course of treatment and patient outcomes. Traditional diagnostic methods primarily rely on visual inspection by experienced burn surgeons. Studies report diagnostic accuracies of around 76% for experts, dropping to nearly 50% for less experienced clinicians. Such inaccuracies can result in suboptimal clinical decisions-delaying vital surgical interventions in severe cases or initiating unnecessary treatments for superficial burns. This diagnostic variability not only compromises patient care but also strains health care resources and increases the likelihood of adverse outcomes. Hence, a more consistent and precise approach to burn classification is urgently needed. OBJECTIVE: The objective is to determine whether a multimodal integrated artificial intelligence (AI) system for accurate classification of burn depth can preserve diagnostic accuracy and provide an important resource when used as part of the electronic medical record (EMR). METHODS: This study used a novel multimodal AI system, integrating digital photographs and ultrasound tissue Doppler imaging (TDI) data to accurately assess burn depth. These imaging modalities were accessed and processed through an EMR system, enabling real-time data retrieval and AI-assisted evaluation. TDI was instrumental in evaluating the biomechanical properties of subcutaneous tissues, using color-coded images to identify burn-induced changes in tissue stiffness and elasticity. The collected imaging data were uploaded to the EMR system (DrChrono), where they were processed by a vision-language model built on GPT-4 architecture. This model received expert-formulated prompts describing how to interpret both digital and TDI images, guiding the AI in making explainable classifications. RESULTS: This study evaluated whether a multimodal AI classifier, designed to identify first-, second-, and third-degree burns, could be effectively applied to imaging data stored within an EMR system. The classifier achieved an overall accuracy of 84.38%, significantly surpassing human performance benchmarks typically cited in the literature. This highlights the potential of the AI model to serve as a robust clinical decision support tool, especially in settings lacking highly specialized expertise. In addition to accuracy, the classifier demonstrated strong performance across multiple evaluation metrics. The classifier's ability to distinguish between burn severities was further validated by the area under the receiver operating characteristic: 0.97 for first-degree, 0.96 for second-degree, and a perfect 1.00 for third-degree burns, each with narrow 95% CIs. CONCLUSIONS: The storage of multimodal imaging data within the EMR, along with the ability for post hoc analysis by AI algorithms, offers significant advancements in burn care, enabling real-time burn depth prediction on currently available data. Using digital photos for superficial burns, easily diagnosed through physical examinations, reduces reliance on TDI, while TDI helps distinguish deep second- and third-degree burns, enhancing diagnostic efficiency.

### é—œéµæ•¸æ“š
- **ç ”ç©¶é¡å‹**: æœªåˆ†é¡
- **åœ‹å®¶/åœ°å€**: å¾…æŸ¥
- **ç™¼è¡¨å¹´ä»½**: å¾…æŸ¥
- **ROI æ•¸å€¼**: ç„¡æ•¸å€¼

### æ ¸å¿ƒå‰µæ–°é»
æœ¬ç ”ç©¶çš„ä¸»è¦å‰µæ–°åœ¨æ–¼ï¼šé«˜

---

## âœï¸ ç¬¬äºŒå¼µï¼šParaphrase å¡

### ç ”ç©¶ç›®çš„èˆ‡æ–¹æ³•
æœ¬ç ”ç©¶èšç„¦æ–¼9é ˜åŸŸï¼Œæ—¨åœ¨æ¢è¨ AI æŠ€è¡“åœ¨é†«ç™‚å ´æ™¯ä¸­çš„æ‡‰ç”¨ã€‚

### ä¸»è¦è²¢ç»
ç ”ç©¶æå‡ºäº†é«˜ï¼Œç‚ºè‡¨åºŠå¯¦è¸æä¾›äº†æ–°çš„è§£æ±ºæ–¹æ¡ˆã€‚

### æŠ€è¡“ç‰¹é»
- æ‡‰ç”¨å ´æ™¯ï¼š9
- å‰µæ–°ç¨‹åº¦ï¼šè©•åˆ† é«˜/10
- æœŸåˆŠå½±éŸ¿ï¼šç™¼è¡¨æ–¼ æœªçŸ¥æœŸåˆŠ

---

## â“ ç¬¬ä¸‰å¼µï¼šå•ç­”å¡

### Q1: é€™é …ç ”ç©¶è§£æ±ºäº†ä»€éº¼å•é¡Œï¼Ÿ
A: æœ¬ç ”ç©¶é‡å°9é ˜åŸŸçš„æŒ‘æˆ°ï¼Œæå‡ºäº†åŸºæ–¼ AI çš„è§£æ±ºæ–¹æ¡ˆã€‚

### Q2: æ¡ç”¨äº†ä»€éº¼æŠ€è¡“æ–¹æ³•ï¼Ÿ
A: é«˜

### Q3: ç ”ç©¶çš„ä¸»è¦æˆæœæ˜¯ä»€éº¼ï¼Ÿ
A: ç ”ç©¶ç²å¾—äº† é«˜ åˆ†çš„å°ˆå®¶è©•åˆ†ï¼Œè­‰æ˜äº†å…¶åœ¨9é ˜åŸŸçš„æ‡‰ç”¨åƒ¹å€¼ã€‚

### Q4: æœ‰å“ªäº›å¯¦éš›æ‡‰ç”¨å ´æ™¯ï¼Ÿ
A: ä¸»è¦æ‡‰ç”¨æ–¼9ï¼Œå¯ç”¨æ–¼æ”¹å–„é†«ç™‚æœå‹™å“è³ªå’Œæ•ˆç‡ã€‚

---

## ğŸ¤” ç¬¬å››å¼µï¼šä¾‹å¤–å¡

### æ½›åœ¨é™åˆ¶
1. **ç ”ç©¶ç¯„åœ**: ç ”ç©¶è©•åˆ†ç‚º é«˜/10ï¼Œå¯èƒ½å­˜åœ¨æ”¹é€²ç©ºé–“
2. **åœ°å€å·®ç•°**: ç ”ç©¶ä¾†è‡ªå¾…æŸ¥ï¼Œå…¶ä»–åœ°å€é©ç”¨æ€§éœ€é©—è­‰
3. **ROI æ•¸æ“š**: ç„¡æ•¸å€¼ï¼Œç¶“æ¿Ÿæ•ˆç›Šè©•ä¼°å¯èƒ½ä¸å®Œæ•´

### éœ€è¦é€²ä¸€æ­¥æ¢è¨çš„å•é¡Œ
1. è©²æŠ€è¡“åœ¨ä¸åŒè¦æ¨¡é†«é™¢çš„é©ç”¨æ€§å¦‚ä½•ï¼Ÿ
2. é•·æœŸä½¿ç”¨çš„æ•ˆæœå’Œç©©å®šæ€§æ˜¯å¦ç¶“éé©—è­‰ï¼Ÿ
3. èˆ‡ç¾æœ‰ç³»çµ±æ•´åˆæ™‚å¯èƒ½é¢è‡¨å“ªäº›æŒ‘æˆ°ï¼Ÿ

### æ‰¹åˆ¤æ€§æ€è€ƒ
- **æŠ€è¡“æˆç†Ÿåº¦**: éœ€è¦è©•ä¼°å¾ç ”ç©¶åˆ°å¯¦éš›éƒ¨ç½²çš„è·é›¢
- **æˆæœ¬æ•ˆç›Š**: ç„¡æ•¸å€¼ï¼Œéœ€è¦æ›´è©³ç´°çš„ç¶“æ¿Ÿåˆ†æ
- **å€«ç†è€ƒé‡**: AI åœ¨9æ‡‰ç”¨æ™‚çš„å€«ç†å’Œéš±ç§å•é¡Œ
- **å¯æ¨å»£æ€§**: ç ”ç©¶çµæœåœ¨å…¶ä»–é†«ç™‚å ´æ™¯çš„é©ç”¨æ€§

---

## ğŸ“š åƒè€ƒè³‡è¨Š
- **PMID**: [40815778](https://pubmed.ncbi.nlm.nih.gov/40815778/)
- **DOI**: https://doi.org/10.2196/68366
- **æœŸåˆŠ**: æœªçŸ¥æœŸåˆŠ
