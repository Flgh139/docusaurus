---
sidebar_position: 27
---

# AIS經典論文 27：什麼不是理論

**English Title**: What theory is not
**中文標題**: 什麼不是理論
**作者**: Sutton, R. I., & Staw, B. M.
**年份**: 1995
**期刊**: Administrative Science Quarterly
**卷期**: 40(3), 371–384
**DOI**: [10.2307/2393788](https://doi.org/10.2307/2393788)

---

## 📌 為什麼每個研究者都應該讀這篇「打臉論文」？

想像你花了一年時間做研究，終於投稿到頂級期刊，結果審稿者回覆：

> **「你的論文沒有理論貢獻 (Lack of theoretical contribution)。」**

你困惑地翻開自己的論文：
- ✅ 有文獻回顧（20 頁）
- ✅ 有研究假設（5 個）
- ✅ 有實證資料（300 份問卷）
- ✅ 有統計分析（SEM 模型）
- ✅ 有討論與結論（10 頁）

**「這樣還不算有理論？！」**

**Sutton & Staw (1995) 用一篇犀利的論文告訴你：**

> **「參考文獻不是理論、資料不是理論、變數不是理論、圖表不是理論、假設也不是理論。**
> **理論是『WHY』——解釋現象背後的因果機制。」**

這篇論文在學術圈掀起巨大爭議，因為它直接挑戰了當時（也是現在）許多「偽理論論文」的寫作慣例：
- ❌ **文獻堆疊型**：引用 100 篇論文，但沒有提出新洞見
- ❌ **假設羅列型**：提出 H1-H10，但不解釋為什麼這些關係會存在
- ❌ **模型展示型**：畫出複雜的路徑圖，但缺乏理論邏輯
- ❌ **資料描述型**：報告統計結果，但不深入解釋機制

**在醫療 AI 研究中，這個問題更加嚴重：**
- 「我們發現 AI 可解釋性正向影響信任（β = 0.45***）」→ **這是實證發現，不是理論**
- 「根據 TAM 理論，易用性影響使用意圖」→ **這是引用理論，不是理論貢獻**
- 「我們提出一個整合模型，結合 10 個變數」→ **這是模型，不是理論**

**Sutton & Staw 的核心主張：**

> **好理論必須回答「WHY」：為什麼 AI 可解釋性會影響信任？背後的心理機制是什麼？**
> **理論 = 解釋因果機制的邏輯故事 (A logical story that explains underlying mechanisms)**

---

## 🎯 NotebookLM 4-Card 學習法

### 核心觀點卡：理論的 5 個「不是」與 1 個「是」

Sutton & Staw (1995) 用「排除法」定義理論——先說清楚「什麼不是理論」，再說「理論是什麼」。

#### ❌ 理論不是什麼？（The Five "NOTs"）

##### ❶ 參考文獻不是理論 (References Are Not Theory)

**常見錯誤**：
```text
錯誤寫法（文獻堆疊）：
「根據 Davis (1989)、Venkatesh (2003)、McKnight (2002) 的研究，
我們認為信任會影響使用意圖。因此提出假設 H1：信任正向影響使用意圖。」

問題診斷：
❌ 只是「引用他人的理論」，沒有提出新的理論洞見
❌ 「因此」之前缺乏邏輯推理，只是「因為別人這樣說，所以我也這樣說」
❌ 這是「文獻回顧」，不是「理論發展」
```

**醫療 AI 研究的典型案例**：

| 錯誤寫法 | 問題 | 正確做法 |
|---------|------|---------|
| 「根據文獻，AI 可解釋性很重要」 | ❌ 只是總結文獻，沒有理論 | ✅ 「當醫師無法理解 AI 推理過程時，會產生『控制感喪失』(Loss of Control)，進而降低信任」|
| 「許多研究指出信任影響採納」 | ❌ 只是引用，沒有解釋機制 | ✅ 「信任降低『感知風險』(Perceived Risk)，因為醫師相信 AI 不會傷害病患」|
| 「過去 20 年有 50 篇論文探討 TAM」| ❌ 文獻數量不等於理論貢獻 | ✅ 「TAM 在醫療情境失效，因為醫師決策受『法律責任』影響，而非『有用性』」|

**Sutton & Staw 的警告**：
> 「大量引用文獻 (Referencing) 只能證明你讀過書，不能證明你有理論貢獻。
> 理論貢獻在於『整合既有知識，提出新洞見』，而非『列舉誰說過什麼』。」

**醫療 AI 研究者應該做的**：
```text
步驟 1：文獻回顧（了解現狀）
「過去研究發現 AI 可解釋性影響信任（r = 0.3-0.6）」

步驟 2：找出理論缺口（Gap）
「但既有研究沒有解釋『為什麼』可解釋性影響信任的心理機制」

步驟 3：提出理論解釋（Theory）
「我們提出『認知控制理論』(Cognitive Control Theory)：
當醫師能理解 AI 推理過程時 → 感知到『可預測性』(Predictability)
→ 恢復『控制感』(Sense of Control) → 降低『不確定性焦慮』→ 提升信任」

步驟 4：理論推導出假設（Hypothesis）
H1: AI 可解釋性正向影響感知可預測性
H2: 感知可預測性正向影響控制感
H3: 控制感負向影響不確定性焦慮
H4: 不確定性焦慮負向影響信任
```

**關鍵差異**：
- ❌ **只引用文獻**：「Davis 說有用性影響意圖」→ 沒有理論貢獻
- ✅ **提出新機制**：「有用性影響意圖，是因為降低了『工作負荷』」→ 有理論貢獻

---

##### ❷ 資料不是理論 (Data Are Not Theory)

**常見錯誤**：
```text
錯誤寫法（資料描述）：
「我們調查了 200 位醫師，發現 65% 認為 AI 可解釋性很重要，
78% 表示會因為可解釋性而提升信任。因此，AI 可解釋性是關鍵因素。」

問題診斷：
❌ 只報告「實證結果」(Empirical Findings)，沒有解釋「為什麼」
❌ 「因此」之前缺乏理論邏輯
❌ 這是「描述性統計」，不是「理論」
```

**醫療 AI 研究的典型案例**：

| 錯誤寫法 | 問題 | 正確做法 |
|---------|------|---------|
| 「65% 醫師認為可解釋性重要」 | ❌ 只是數據，沒有解釋 | ✅ 「醫師重視可解釋性，因為醫療決策需要『法律責任歸屬』(Legal Accountability)」|
| 「使用頻率與滿意度相關 r = 0.7」| ❌ 相關不等於因果 | ✅ 「使用頻率提升滿意度，因為『熟悉效應』(Mere Exposure Effect) 降低認知負荷」|
| 「AI 診斷準確率達 95%」 | ❌ 只是績效指標 | ✅ 「高準確率未必提升信任，因為醫師更在意『可解釋性』而非『準確性』」|

**Sutton & Staw 的關鍵區分**：

| 類型 | 定義 | 範例 | 是否為理論？ |
|------|------|------|------------|
| **資料 (Data)** | 觀察到的現象或模式 | 「AI 可解釋性與信任相關 r = 0.6」 | ❌ 不是理論 |
| **實證發現 (Empirical Findings)** | 資料分析的結果 | 「可解釋性對信任有正向影響（β = 0.45***）」| ❌ 不是理論 |
| **理論解釋 (Theoretical Explanation)** | 解釋為什麼會有這個結果 | 「可解釋性提升信任，因為降低『不確定性焦慮』」| ✅ 這是理論 |

**醫療 AI 的經典反例**：

```text
研究 A（只有資料，沒有理論）：
「我們收集了 5,000 筆 AI 診斷記錄，發現醫師採納率為 60%。
當 AI 提供解釋時，採納率提升至 75%（p < 0.001）。」

→ 問題：只報告「發生什麼」(What)，沒有解釋「為什麼」(Why)

研究 B（有理論）：
「我們提出『信任修復理論』(Trust Repair Theory)：
當醫師對 AI 產生懷疑時，可解釋性扮演『信任修復機制』的角色。
具體來說，解釋讓醫師看到 AI 的『推理過程』→ 發現 AI 考慮了『臨床相關因素』
→ 重新評估 AI 的『能力』→ 修復受損的信任 → 提升採納率。」

→ 有理論：解釋了「可解釋性 → 採納率」背後的心理機制
```

**Sutton & Staw 的金句**：
> 「資料告訴你『發生了什麼』(What happened)，
> 理論告訴你『為什麼會發生』(Why it happened) 和『未來會怎樣』(What will happen next)。」

---

##### ❸ 變數列表不是理論 (Lists of Variables Are Not Theory)

**常見錯誤**：
```text
錯誤寫法（變數羅列）：
「本研究探討以下變數對 AI 採納的影響：
1. 知覺有用性
2. 知覺易用性
3. 信任
4. 可解釋性
5. 隱私關注
6. 社會影響
7. 促成條件
8. 工作相關性
9. 主觀規範
10. 自我效能」

問題診斷：
❌ 只是「列出變數」，沒有解釋「為什麼這些變數重要」
❌ 缺乏「變數之間的理論關係」
❌ 這是「研究架構」(Research Framework)，不是「理論」
```

**醫療 AI 研究的典型案例**：

| 錯誤做法 | 問題 | 正確做法 |
|---------|------|---------|
| 列出 10 個影響因素 | ❌ 只是變數清單 | ✅ 解釋「為什麼這些因素重要」+「它們如何交互作用」|
| 「我們整合了 5 個理論模型」| ❌ 拼湊變數，缺乏邏輯 | ✅ 「我們提出新機制，解釋為何醫療情境需要這些因素」|
| 畫出包含所有變數的大圖 | ❌ 複雜≠理論貢獻 | ✅ 「我們簡化模型，聚焦核心因果機制」|

**Sutton & Staw 的診斷**：
> 「列出變數就像列出食材，但理論是『食譜』——你必須說明：
> - 為什麼需要這些食材（變數）？
> - 它們如何交互作用（因果關係）？
> - 最終產生什麼結果（理論預測）？」

**醫療 AI 的錯誤案例**：

```text
研究 C（變數列表型）：
「本研究探討影響醫師 AI 採納的因素：
自變數：有用性、易用性、可解釋性、信任、隱私關注
調節變數：年資、專科、醫院規模
依變數：使用意圖、實際使用」

→ 問題：只是列出變數，沒有理論故事

研究 D（有理論）：
「我們提出『醫療 AI 雙路徑理論』(Dual-Path Theory of Medical AI Adoption)：
醫師採納 AI 經由兩條路徑：

路徑 1：效率路徑 (Efficiency Path)
有用性 → 感知效率提升 → 使用意圖
（機制：時間壓力下的工具性動機）

路徑 2：信任路徑 (Trust Path)
可解釋性 → 理解 AI 推理 → 信任 → 使用意圖
（機制：醫療責任下的風險控制動機）

關鍵理論預測：
- 高工作負荷情境：路徑 1 主導（效率優先）
- 高風險決策情境：路徑 2 主導（信任優先）」

→ 有理論：解釋了「為什麼」需要這兩條路徑，以及「何時」哪條路徑主導
```

**關鍵差異**：
- ❌ **變數列表**：「A、B、C、D、E 都影響 Y」→ 沒有理論
- ✅ **理論機制**：「A 透過降低 X，進而影響 Y；但 B 透過提升 Z，也影響 Y」→ 有理論

---

##### ❹ 圖表不是理論 (Diagrams Are Not Theory)

**常見錯誤**：
```text
錯誤寫法（模型展示）：
「圖 1 展示本研究的理論模型：
[一個包含 10 個方框、20 條箭頭的複雜路徑圖]」

問題診斷：
❌ 圖表只是「視覺化工具」，不是理論本身
❌ 複雜的箭頭不等於深刻的理論
❌ 缺乏「為什麼這些箭頭存在」的文字解釋
```

**Sutton & Staw 的核心主張**：

> 「圖表是理論的『視覺摘要』(Visual Summary)，不是理論本身。
> 好理論必須用『文字敘述』(Narrative) 解釋因果邏輯，圖表只是輔助。」

**醫療 AI 研究的典型錯誤**：

| 錯誤做法 | 問題 | 正確做法 |
|---------|------|---------|
| 畫出複雜的整合模型圖 | ❌ 視覺複雜≠理論深度 | ✅ 用文字解釋每條路徑背後的「WHY」|
| 只有圖，沒有理論敘述 | ❌ 讀者無法理解邏輯 | ✅ 先寫理論故事，再用圖總結 |
| 所有變數都連來連去 | ❌ 缺乏理論焦點 | ✅ 聚焦核心因果機制 |

**實例對比**：

```text
錯誤範例（圖表型）：
┌──────────┐      ┌──────────┐      ┌──────────┐
│ 有用性   │──────→│ 使用意圖 │──────→│ 實際使用 │
└──────────┘      └──────────┘      └──────────┘
     ↑                  ↑                  ↑
┌──────────┐      ┌──────────┐      ┌──────────┐
│ 易用性   │──────→│ 信任     │──────→│ 滿意度   │
└──────────┘      └──────────┘      └──────────┘
     ↑                  ↑
┌──────────┐      ┌──────────┐
│可解釋性  │──────→│ 隱私關注 │
└──────────┘      └──────────┘

文字說明：「圖 1 顯示本研究的理論模型。」

→ 問題：圖表很複雜，但沒有解釋「為什麼」這些箭頭存在

正確範例（有理論）：
理論敘述：
「我們提出『認知負荷調節理論』(Cognitive Load Moderation Theory)：

核心機制：
當醫師工作負荷低時 → 有時間深入思考 → 更在意『可解釋性』→ 透過理解 AI 推理建立信任
當醫師工作負荷高時 → 沒時間深入思考 → 更在意『易用性』→ 透過快速操作提升意圖

理論預測：
- 低負荷情境：可解釋性 → 信任 的路徑強度 β = 0.6***
- 高負荷情境：易用性 → 意圖 的路徑強度 β = 0.7***

圖 1 視覺化上述理論邏輯。」

→ 有理論：先解釋機制，再用圖總結
```

**Sutton & Staw 的測試標準**：
> 「遮住圖表，只看文字，能否理解理論邏輯？
> 如果不能，代表你的理論『藏在圖裡』，而非『寫在文字裡』。」

---

##### ❺ 假設不是理論 (Hypotheses Are Not Theory)

**常見錯誤**：
```text
錯誤寫法（假設羅列）：
「基於文獻回顧，我們提出以下假設：
H1: 知覺有用性正向影響使用意圖
H2: 知覺易用性正向影響使用意圖
H3: 信任正向影響使用意圖
H4: 可解釋性正向影響信任
H5: 隱私關注負向影響信任」

問題診斷：
❌ 假設只是「預測結果」，不是「解釋機制」
❌ 缺乏「為什麼這些關係存在」的理論邏輯
❌ 這是「待驗證的命題」，不是「理論」
```

**Sutton & Staw 的核心區分**：

| 類型 | 定義 | 範例 | 是否為理論？ |
|------|------|------|------------|
| **假設 (Hypothesis)** | 預測兩個變數的關係 | H1: 可解釋性正向影響信任 | ❌ 不是理論 |
| **理論 (Theory)** | 解釋為什麼這個關係存在 | 可解釋性提升信任，因為降低不確定性焦慮 | ✅ 這是理論 |

**醫療 AI 研究的典型錯誤**：

```text
錯誤範例（假設堆疊型）：
「基於 TAM，我們提出：
H1: 知覺有用性正向影響醫師的 AI 使用意圖
H2: 知覺易用性正向影響醫師的 AI 使用意圖
H3: 知覺易用性正向影響知覺有用性」

→ 問題：
1. 只是「重複 TAM 的假設」，沒有理論貢獻
2. 沒有解釋「為什麼」醫療情境會有這些關係
3. 缺乏「新的理論機制」

正確範例（有理論）：
「我們提出『醫療責任理論』(Medical Liability Theory)：

理論論證：
醫療決策與一般消費決策的關鍵差異在於『法律責任』。
當醫師採納 AI 建議時，若出現醫療疏失，醫師需承擔法律責任。
因此，醫師的 AI 採納決策，不僅考慮『有用性』（效率提升），
更關鍵的是『責任歸屬』（能否解釋決策依據）。

理論推導：
前提 1：醫師需對醫療決策負法律責任
前提 2：AI 建議若無法解釋，醫師無法向法院說明決策依據
前提 3：醫師會規避無法說明依據的決策（風險趨避）
結論：在醫療情境，『可解釋性』的重要性 > 『有用性』

假設推導（基於理論）：
H1: 可解釋性對使用意圖的影響 > 有用性對使用意圖的影響
H2: 高風險決策情境，可解釋性的影響更強（調節效果）
H3: 資深醫師（法律意識更強），可解釋性的影響更強（調節效果）」

→ 有理論：
1. 提出新機制（法律責任）
2. 解釋為何醫療情境不同
3. 推導出可測試的預測
```

**Sutton & Staw 的警告**：
> 「假設是理論的『可測試預測』(Testable Predictions)，不是理論本身。
> 沒有理論邏輯支撐的假設，就像沒有地基的房子——看似完整，實則空洞。」

---

#### ✅ 理論是什麼？（What Theory IS）

在批評完「五個不是」之後，Sutton & Staw 給出「理論」的正面定義：

> **理論 = 解釋『為什麼』變數之間存在某種關係的『邏輯故事』**
> **Theory = A logical story that explains why variables are related**

**理論的核心要素**：

```text
理論必須包含：

1. 因果邏輯 (Causal Logic)
   → 解釋「為什麼」A 會影響 B

2. 潛在機制 (Underlying Mechanisms)
   → 說明「透過什麼過程」A 影響 B

3. 理論邊界 (Boundary Conditions)
   → 界定「在什麼情境下」這個機制有效

4. 可測試預測 (Testable Predictions)
   → 推導出「如果理論正確，會看到什麼結果」
```

**醫療 AI 的理論範例**：

```text
研究問題：為什麼 AI 可解釋性影響醫師信任？

理論論證（WHY）：
「我們提出『控制感恢復理論』(Control Restoration Theory)：

前提 1：醫師的專業認同建立在『診斷控制感』上
當醫師使用傳統方法診斷時，完全掌握推理過程 → 高度控制感

前提 2：黑箱 AI 威脅控制感
當 AI 直接給出診斷建議，醫師無法理解推理過程 → 控制感喪失

前提 3：控制感喪失引發焦慮與抗拒
心理學研究顯示，控制感喪失會引發『習得無助感』→ 降低信任

機制：可解釋性如何恢復控制感
當 AI 提供解釋時：
步驟 1：醫師看到 AI 考慮的特徵（如『腫瘤大小』『邊界清晰度』）
步驟 2：醫師驗證這些特徵是否符合醫學知識
步驟 3：醫師感知到『AI 推理過程可理解』→ 恢復控制感
步驟 4：控制感恢復 → 降低焦慮 → 提升信任

理論邊界：
- 適用情境：高風險醫療決策（如癌症診斷）
- 不適用情境：低風險行政任務（如排班建議）
- 調節變數：醫師專業自主性（自主性越高，控制感需求越強）

可測試預測：
H1: AI 可解釋性正向影響感知控制感
H2: 感知控制感正向影響信任
H3: 可解釋性 → 控制感 → 信任（中介效果）
H4: 高風險決策情境，可解釋性的影響更強（調節效果）
H5: 高專業自主性醫師，可解釋性的影響更強（調節效果）」
```

**這就是理論**：
- ✅ 解釋了「WHY」（控制感機制）
- ✅ 說明了「HOW」（可解釋性 → 控制感 → 信任）
- ✅ 界定了「WHEN」（高風險、高自主性情境）
- ✅ 推導出「可測試預測」（5 個假設）

---

### 改寫卡：醫療 AI 研究的理論建構實作案例

#### 📋 研究情境

某醫學資訊博士生小王，正在撰寫關於「AI 輔助診斷系統採納」的論文。他的初稿被指導教授退回，並附上 Sutton & Staw (1995) 這篇論文，批註：

> **「你的論文沒有理論貢獻，只是 TAM 的重複應用。請重新思考你的理論故事。」**

小王困惑不已：「我明明引用了 20 篇理論文獻，提出了 8 個假設，還畫了複雜的研究模型，怎麼會沒有理論？」

讓我們看看小王的論文如何從「沒有理論」變成「有理論貢獻」。

---

#### 🔴 初稿版本（沒有理論）

**研究架構**：

```text
第一章：緒論
- 研究背景：AI 在醫療的應用
- 研究動機：了解醫師採納 AI 的因素
- 研究目的：驗證 TAM 在醫療 AI 情境的適用性

第二章：文獻回顧（20 頁）
- 技術接受模型（TAM）
  - Davis (1989) 提出 TAM
  - Venkatesh (2003) 擴展為 UTAUT
  - 過去 30 年有 200 篇論文應用 TAM
- AI 可解釋性
  - Miller (2019) 定義可解釋性
  - Doshi-Velez (2017) 提出評估方法
  - 醫療情境需要可解釋性（10 篇文獻）
- 信任
  - McKnight (2002) 的信任模型
  - 信任影響技術採納（15 篇文獻）

第三章：研究方法
研究模型（圖 1）：
┌──────────┐      ┌──────────┐      ┌──────────┐
│ 有用性   │──────→│          │──────→│          │
└──────────┘      │          │      │ 實際使用 │
┌──────────┐      │ 使用意圖 │      │          │
│ 易用性   │──────→│          │──────→│          │
└──────────┘      └──────────┘      └──────────┘
┌──────────┐            ↑
│可解釋性  │────────────┘
└──────────┘

研究假設：
H1: 知覺有用性正向影響使用意圖
H2: 知覺易用性正向影響使用意圖
H3: AI 可解釋性正向影響使用意圖
H4: 使用意圖正向影響實際使用

第四章：資料分析
- 樣本：300 位醫師
- 分析方法：PLS-SEM
- 結果：H1-H4 全部支持

第五章：討論與結論
- 本研究驗證了 TAM 在醫療 AI 情境的適用性
- AI 可解釋性是重要因素
- 建議：醫院應導入可解釋的 AI 系統
```

**指導教授的批評**：

| Sutton & Staw 的標準 | 小王的問題 |
|---------------------|-----------|
| ❌ 參考文獻不是理論 | 文獻回顧 20 頁，但只是「總結誰說過什麼」，沒有新洞見 |
| ❌ 資料不是理論 | 只報告「300 位醫師的問卷結果」，沒有解釋機制 |
| ❌ 變數列表不是理論 | 只是列出「有用性、易用性、可解釋性」，沒有理論故事 |
| ❌ 圖表不是理論 | 研究模型只是「變數+箭頭」，缺乏因果邏輯解釋 |
| ❌ 假設不是理論 | H1-H4 只是預測，沒有說明「為什麼」這些關係存在 |

**核心問題**：
> 這篇論文是「TAM 的重複應用」，沒有提出新的理論機制。
> 只是「把既有理論套用到新情境」，不是「理論貢獻」。

---

#### 🟢 修正版本（有理論貢獻）

小王重新思考後，提出全新的理論論證：

**核心理論問題**：
> 「為什麼 TAM 預測的『有用性主導』在醫療 AI 情境失效？
> 為什麼醫師更在意『可解釋性』而非『有用性』？」

**理論建構過程**：

##### **步驟 1：找出理論缺口（Gap）**

```text
既有理論的預測（TAM）：
「有用性」是技術採納的最強預測因子

醫療 AI 的實證矛盾：
- 小王的預試資料（N=50）：可解釋性 → 意圖（β = 0.61***）
                         有用性 → 意圖（β = 0.28*）
- 文獻證據：多項研究發現醫師抗拒「高準確但無法解釋」的 AI

理論缺口：
TAM 無法解釋「為什麼醫療情境，可解釋性 > 有用性」
```

##### **步驟 2：提出新理論機制**

```text
理論名稱：「醫療責任驅動理論」(Medical Liability-Driven Theory)

核心論證：
醫療決策與一般工作任務的根本差異在於「法律責任」。
當醫師採納 AI 建議導致醫療疏失時，醫師需承擔刑事/民事責任。

理論機制（WHY）：

機制 1：責任歸屬需求 (Accountability Requirement)
前提：醫師需向法院/病患家屬解釋醫療決策依據
過程：黑箱 AI → 無法說明決策依據 → 法律風險 → 規避使用
結果：可解釋性成為「必要條件」，有用性是「充分條件」

機制 2：專業自主性威脅 (Professional Autonomy Threat)
前提：醫師的專業認同建立在「獨立診斷能力」上
過程：黑箱 AI → 醫師變成「橡皮圖章」→ 專業自主性喪失 → 抗拒
結果：可解釋性讓醫師「保留最終決策權」→ 維護專業自主

機制 3：信任建立路徑差異 (Differential Trust-Building Paths)
一般情境：信任建立於「使用經驗」（試用後發現有用 → 信任）
醫療情境：信任建立於「推理透明」（理解邏輯後才敢使用）
結果：醫療 AI 需要「先信任，後使用」，而非「先使用，後信任」

理論邊界（WHEN）：
適用情境：
✅ 高風險醫療決策（如癌症診斷、手術決策）
✅ 高責任歸屬情境（如急診、ICU）
✅ 高專業自主性科別（如放射科、病理科）

不適用情境：
❌ 低風險行政任務（如排班、掛號）
❌ 無法律責任情境（如健康建議 App）
❌ 輔助角色（如文獻檢索工具）

調節變數：
- 決策風險高低（風險越高，可解釋性需求越強）
- 醫師年資（資深醫師法律意識更強，更重視可解釋性）
- 醫療糾紛經驗（曾遭醫療訴訟者，更重視責任歸屬）
```

##### **步驟 3：推導可測試假設**

```text
基於「醫療責任驅動理論」，推導以下假設：

主效果假設（基於機制 1）：
H1a: AI 可解釋性正向影響感知責任可歸屬性
H1b: 感知責任可歸屬性正向影響使用意圖
H1c: 可解釋性透過責任可歸屬性影響意圖（中介效果）

比較假設（理論的關鍵預測）：
H2: 可解釋性對使用意圖的影響 > 有用性對使用意圖的影響
    （這與 TAM 的預測相反！）

調節效果假設（基於理論邊界）：
H3a: 決策風險越高，可解釋性的影響越強
H3b: 醫師年資越高，可解釋性的影響越強
H3c: 有醫療糾紛經驗者，可解釋性的影響更強

競爭機制檢驗（排除替代解釋）：
H4: 可解釋性對意圖的影響，主要透過「責任歸屬」而非「信任」
    （如果只是信任問題，一般情境也該有類似效果）
```

##### **步驟 4：理論故事的完整敘述**

**修正後的論文架構**：

```text
第一章：緒論
研究問題：
「為什麼 TAM 預測的『有用性主導』在醫療 AI 情境失效？」

研究貢獻：
1. 理論貢獻：提出「醫療責任驅動理論」，解釋醫療情境的獨特機制
2. 實務貢獻：指出醫療 AI 設計的優先順序（可解釋性 > 有用性）

第二章：理論發展（不是文獻回顧）
2.1 既有理論的侷限
- TAM 假設「有用性主導」，但醫療情境有法律責任考量
- 信任理論假設「經驗建立信任」，但醫療不允許「試錯」

2.2 醫療責任驅動理論
- 機制 1：責任歸屬需求
- 機制 2：專業自主性威脅
- 機制 3：信任建立路徑差異

2.3 理論推導與假設
（詳見步驟 3）

第三章：研究方法
3.1 研究設計
- 主研究：問卷調查（300 位醫師）
- 補充研究：實驗法（驗證因果機制）

3.2 測量工具
- 新構念：「感知責任可歸屬性」（自行開發量表）
- 既有構念：有用性、易用性、信任（引用既有量表）

第四章：資料分析
4.1 假設檢驗
- H1a-H1c：中介效果成立 ✓
- H2：可解釋性影響（β = 0.58***）> 有用性影響（β = 0.31*）✓
- H3a-H3c：調節效果成立 ✓
- H4：責任歸屬路徑顯著，信任路徑不顯著 ✓

4.2 競爭理論檢驗
- 模型 A（TAM）：R² = 0.42
- 模型 B（責任驅動理論）：R² = 0.67（顯著優於模型 A）

第五章：討論
5.1 理論貢獻
- 挑戰 TAM 的「有用性主導」假設
- 提出醫療情境的獨特機制（法律責任）
- 擴展技術接受理論的邊界條件

5.2 實務啟示
- AI 開發優先順序：可解釋性 > 準確性 > 易用性
- 高風險決策需要「推理透明」，而非「結果準確」
```

---

#### 📊 理論貢獻對比表

| 標準 | 初稿版本 | 修正版本 |
|------|---------|---------|
| **理論問題** | ❌ 沒有明確的理論問題 | ✅ 「為什麼 TAM 在醫療情境失效？」|
| **理論機制** | ❌ 只引用 TAM，沒有新機制 | ✅ 提出「責任歸屬」「專業自主」等新機制 |
| **因果邏輯** | ❌ 只有「A → B」的預測 | ✅ 解釋「A 透過 X 影響 B，因為 Y」|
| **理論邊界** | ❌ 沒有界定適用情境 | ✅ 明確界定「高風險、高責任」情境 |
| **可測試性** | ❌ 假設只是重複 TAM | ✅ 推導出「反直覺」的預測（可解釋性 > 有用性）|
| **理論挑戰** | ❌ 驗證既有理論 | ✅ 挑戰既有理論的邊界 |
| **新構念** | ❌ 沒有新構念 | ✅ 提出「感知責任可歸屬性」|
| **替代解釋** | ❌ 沒有排除其他解釋 | ✅ 檢驗「責任」vs.「信任」的競爭機制 |

---

### 問答卡：理論建構的常見困惑

#### Q1: 「整合理論」算不算理論貢獻？把 5 個理論模型整合成一個大模型？

**A**: Sutton & Staw 的回答是：**「整合不等於理論貢獻」**，除非你提出新的整合邏輯。

**常見錯誤**：
```text
錯誤做法（拼湊型整合）：
「本研究整合 TAM、TTF、信任理論、隱私關注理論、UTAUT，
提出『醫療 AI 採納整合模型』，包含 12 個構念、25 條路徑。」

問題：
❌ 只是「把不同理論的變數放在一起」
❌ 沒有解釋「為什麼需要整合這些理論」
❌ 沒有提出「整合後產生什麼新洞見」
❌ 複雜度提升，但理論深度沒有提升
```

**正確做法（有理論的整合）**：

```text
研究問題：
「為什麼醫療 AI 採納同時受到『效率動機』和『風險控制動機』驅動？
既有理論（TAM、信任理論）各只解釋單一動機，如何整合？」

理論貢獻：
「我們提出『雙動機平衡理論』(Dual-Motive Balancing Theory)：

新機制：
醫師面臨『效率-風險權衡困境』(Efficiency-Risk Tradeoff)：
- 採納 AI → 提升效率（正面）+ 增加風險（負面）
- 不採納 AI → 保持安全（正面）+ 降低效率（負面）

整合邏輯：
TAM 的『有用性』= 效率動機（提升診斷速度）
信任理論的『信任』= 風險控制動機（降低醫療疏失風險）

關鍵新洞見：
兩種動機並非「加總關係」，而是「權衡關係」：
- 當效率壓力高時 → 效率動機主導 → TAM 路徑強化
- 當風險感知高時 → 風險控制動機主導 → 信任路徑強化

理論預測（區別於簡單整合）：
H1: 高工作負荷情境，有用性的影響增強（效率動機主導）
H2: 高風險決策情境，信任的影響增強（風險控制動機主導）
H3: 效率壓力 × 風險感知 的交互作用影響採納決策」
```

**Sutton & Staw 的判斷標準**：

| 整合類型 | 理論貢獻 | 範例 |
|---------|---------|------|
| **拼湊型整合** | ❌ 無貢獻 | 「我們整合了 10 個理論」（但沒有新邏輯）|
| **加總型整合** | ⚠️ 微弱貢獻 | 「A 和 B 都影響 Y」（只是加總，沒有交互作用）|
| **機制型整合** | ✅ 有貢獻 | 「A 透過 X 影響 Y，B 透過 Z 影響 Y，且 X 與 Z 交互作用」|
| **權衡型整合** | ✅ 強貢獻 | 「A 與 B 是權衡關係，情境 C 決定哪個主導」|

**醫療 AI 的典型案例**：

```text
拼湊型整合（無貢獻）：
「我們整合 TAM、UTAUT、信任理論、隱私關注理論，
提出包含『有用性、易用性、信任、隱私關注、社會影響』的整合模型。」

→ 問題：只是把 5 個理論的變數放在一起，沒有新邏輯

機制型整合（有貢獻）：
「我們整合 TAM 和隱私關注理論，提出『隱私-效用權衡機制』：
當 AI 需要存取敏感病歷時，醫師面臨困境：
- 提供完整資料 → AI 更準確（有用性提升）+ 隱私風險增加
- 限制資料存取 → 保護隱私 + AI 準確度下降（有用性降低）

關鍵新洞見：
隱私關注不是『降低採納意圖』，而是『改變醫師使用 AI 的方式』：
- 高隱私關注醫師 → 選擇『本地部署 AI』（而非雲端 AI）
- 高隱私關注醫師 → 採用『差分隱私技術』（而非拒絕使用）」

→ 有貢獻：提出「權衡機制」和「行為適應策略」
```

---

#### Q2: 質性研究（如個案研究、紮根理論）需要理論嗎？還是只有量化研究需要理論？

**A**: Sutton & Staw 明確指出：**所有類型的研究都需要理論**，但理論呈現方式不同。

**量化研究的理論**：
- 演繹邏輯 (Deductive)：從理論推導假設，用資料驗證
- 理論在前，資料在後

**質性研究的理論**：
- 歸納邏輯 (Inductive)：從資料中發現模式，建立理論
- 資料在前，理論在後

**醫療 AI 質性研究的理論貢獻範例**：

```text
研究問題：
「為什麼某些醫師抗拒 AI，即使 AI 準確率高達 95%？」

研究方法：
- 深度訪談 20 位放射科醫師（10 位採納 AI，10 位抗拒 AI）
- 紮根理論分析

資料發現（歸納）：
訪談逐字稿中反覆出現的主題：
1. 抗拒者常提到「我不知道 AI 怎麼想的」（出現 15 次）
2. 抗拒者常提到「我怕出事說不清楚」（出現 12 次）
3. 抗拒者常提到「我的專業價值在哪裡」（出現 9 次）

理論建構（從資料到理論）：
基於上述主題，我們提出「專業認同威脅理論」：

理論機制：
當醫師使用黑箱 AI 時，經歷三階段心理歷程：

階段 1：認知失控 (Cognitive Loss of Control)
「我不知道 AI 怎麼想的」→ 無法理解 AI 推理 → 認知控制感喪失

階段 2：責任焦慮 (Liability Anxiety)
「我怕出事說不清楚」→ 擔心無法向法院解釋 → 法律責任焦慮

階段 3：專業認同威脅 (Professional Identity Threat)
「我的專業價值在哪裡」→ 感覺被 AI 取代 → 專業認同危機

抗拒機制：
認知失控 + 責任焦慮 + 認同威脅 → 心理防衛 → 抗拒 AI

理論邊界：
- 適用於「診斷型 AI」（威脅核心專業能力）
- 不適用於「輔助型 AI」（如影像增強工具）

可測試預測（質性理論也能推導假設）：
H1: 診斷型 AI 的抗拒程度 > 輔助型 AI
H2: 高專業認同醫師的抗拒程度更高
H3: 提供可解釋性能緩解三階段威脅
```

**Sutton & Staw 的標準（適用於質性研究）**：

| 質性研究類型 | 理論貢獻判斷 |
|------------|------------|
| **描述型個案** | ❌ 只描述「發生什麼」→ 無理論貢獻 |
| **解釋型個案** | ✅ 解釋「為什麼發生」→ 有理論貢獻 |
| **資料呈現型** | ❌ 只呈現訪談引文 → 無理論貢獻 |
| **理論建構型** | ✅ 從資料歸納出理論機制 → 有理論貢獻 |

**醫療 AI 質性研究的錯誤案例**：

```text
錯誤範例（描述型，無理論）：
「我們訪談了 20 位醫師，發現：
- 60% 認為 AI 可解釋性很重要
- 40% 擔心 AI 取代工作
- 80% 希望保留最終決策權

結論：醫師重視可解釋性和決策自主權。」

→ 問題：只是「報告訪談結果」，沒有理論

正確範例（理論建構型，有理論）：
「我們訪談了 20 位醫師，歸納出『信任修復循環理論』：

理論機制：
醫師對 AI 的信任建立是『循環歷程』，而非『線性歷程』：

循環 1：初始信任建立
醫師基於「演算法聲譽」建立初始信任 → 嘗試使用 AI

循環 2：信任破裂
AI 給出「反直覺建議」→ 醫師驗證後發現 AI 錯誤 → 信任破裂

循環 3：信任修復
醫師要求「AI 解釋推理過程」→ 發現 AI 考慮了「醫師忽略的因素」
→ 重新評估 AI 能力 → 信任修復

關鍵發現：
可解釋性在「信任修復階段」最重要，而非「初始信任階段」

理論預測：
- 經歷過「信任破裂」的醫師，更重視可解釋性
- 可解釋性的價值在「修復信任」，而非「建立信任」」

→ 有理論：提出「循環機制」和「可解釋性的作用時機」
```

---

#### Q3: 我的研究發現與既有理論「矛盾」，這算不算理論貢獻？

**A**: Sutton & Staw 認為：**矛盾發現本身不是理論，但『解釋矛盾的機制』是理論貢獻**。

**醫療 AI 的典型矛盾發現**：

```text
矛盾現象：
既有理論（TAM）預測：AI 準確率越高 → 有用性越高 → 採納率越高

你的資料發現：AI 準確率 95% → 採納率 60%
              AI 準確率 85% + 可解釋 → 採納率 80%

矛盾：為什麼「低準確但可解釋」的採納率 > 「高準確但黑箱」？
```

**錯誤做法（只報告矛盾，沒有理論）**：

```text
「我們發現醫師偏好『準確率較低但可解釋』的 AI，
這與 TAM 的預測矛盾。這顯示醫療情境有其特殊性。」

→ 問題：
❌ 只指出「矛盾存在」，沒有解釋「為什麼矛盾」
❌ 「醫療情境特殊」是廢話，沒有理論內容
❌ 沒有提出「新機制」來解釋矛盾
```

**正確做法（解釋矛盾機制，有理論）**：

```text
理論問題：
「為什麼醫師偏好『低準確+可解釋』勝過『高準確+黑箱』？」

理論解釋：
「我們提出『錯誤可修復性理論』(Error Correctability Theory)：

核心邏輯：
醫師評估 AI 的標準不是『是否會犯錯』，而是『犯錯後能否修正』。

機制 1：黑箱 AI 的不可修復性
當黑箱 AI 犯錯時：
醫師無法理解錯在哪裡 → 無法調整 AI 輸入 → 只能「全盤接受」或「全盤拒絕」
→ 醫師選擇「拒絕」（規避風險）

機制 2：可解釋 AI 的可修復性
當可解釋 AI 犯錯時：
醫師看到 AI 推理過程 → 發現「AI 遺漏了某項檢查數據」
→ 補充數據後重新運算 → AI 給出正確建議 → 醫師信任提升

關鍵洞見：
可解釋性的價值不在「提升準確率」，而在「降低錯誤的不可修復性」

理論預測：
H1: 可解釋性對採納的影響，在「AI 曾犯錯」情境更強（調節效果）
H2: 醫師更偏好「80% 準確 + 可解釋」勝過「95% 準確 + 黑箱」
H3: 「錯誤可修復感」中介「可解釋性 → 採納」的關係」
```

**Sutton & Staw 的判斷**：

| 處理矛盾的方式 | 理論貢獻 |
|--------------|---------|
| **只報告矛盾** | ❌ 無貢獻（只是實證發現）|
| **提出「情境特殊」** | ❌ 無貢獻（沒有具體機制）|
| **提出新機制解釋矛盾** | ✅ 有貢獻（這是理論建構）|
| **修正既有理論的邊界** | ✅ 有貢獻（理論精緻化）|

---

#### Q4: 「中介效果」或「調節效果」算不算理論貢獻？

**A**: Sutton & Staw 的回答：**統計模型不是理論，但『解釋為什麼存在中介/調節的機制』是理論**。

**常見錯誤（只有統計，沒有理論）**：

```text
錯誤寫法：
「我們發現『信任』中介『可解釋性 → 採納』的關係。
可解釋性透過信任影響採納（間接效果 β = 0.35***）。」

問題：
❌ 只報告「統計結果」
❌ 沒有解釋「為什麼信任是中介變數」
❌ 沒有解釋「可解釋性如何影響信任」的心理過程
```

**正確做法（有理論機制）**：

```text
理論論證：
「我們提出『認知-情感雙階段理論』：

階段 1：認知處理階段（可解釋性 → 信任）
當醫師看到 AI 解釋時：
步驟 1：認知評估 → 檢查 AI 是否考慮「臨床相關特徵」
步驟 2：能力判斷 → 如果 AI 考慮了正確特徵 → 評估 AI「有能力」
步驟 3：信任形成 → 能力信任建立

階段 2：情感-行為階段（信任 → 採納）
當醫師信任 AI 時：
步驟 1：風險感知降低 → 相信 AI 不會傷害病患
步驟 2：採納意願提升 → 願意嘗試使用 AI
步驟 3：實際採納 → 在臨床決策中採納 AI 建議

為什麼是「中介」而非「直接效果」？
可解釋性本身不直接影響採納，必須先建立信任（中介變數）
因為：醫師不會因為「理解 AI」就使用它，而是因為「信任 AI」才使用

理論預測：
H1: 可解釋性 → 能力信任（直接效果）
H2: 能力信任 → 採納（直接效果）
H3: 可解釋性 → 採納（完全中介，直接效果不顯著）」
```

**調節效果的理論化範例**：

```text
統計發現（無理論）：
「決策風險調節『可解釋性 → 採納』的關係。
高風險情境：β = 0.68***
低風險情境：β = 0.32**」

理論解釋（有理論）：
「我們提出『風險調節機制』(Risk Moderation Mechanism)：

理論邏輯：
可解釋性的價值在於『降低決策風險感』。
當決策風險本身就低時，可解釋性的邊際價值有限。
當決策風險很高時，可解釋性成為「必要條件」。

機制：
低風險決策（如常見感冒）：
醫師風險感知低 → 即使 AI 無法解釋，風險仍可接受 → 可解釋性需求低

高風險決策（如癌症診斷）：
醫師風險感知高 → AI 無法解釋 → 風險無法評估 → 拒絕採納
AI 可解釋 → 風險可評估 → 風險可接受 → 採納

理論預測：
H1: 高風險情境，可解釋性對採納的影響更強（正向調節）
H2: 當可解釋性為 0 時，高風險情境的採納率 < 低風險情境
H3: 當可解釋性為高時，高風險與低風險情境的採納率差異縮小」
```

**Sutton & Staw 的標準**：

| 中介/調節報告方式 | 理論貢獻 |
|-----------------|---------|
| 只報告統計結果 | ❌ 無貢獻 |
| 報告統計 + 簡單解釋 | ⚠️ 微弱貢獻 |
| 報告統計 + 詳細機制 + 理論邊界 | ✅ 有貢獻 |

---

### 例外卡：什麼時候「沒有新理論」也能發表？

Sutton & Staw (1995) 雖然強調理論貢獻，但也承認某些情況下，「沒有新理論」的研究仍有價值。

#### 例外 1：理論複製研究 (Replication Study)

**定義**: 在新樣本、新情境重複驗證既有理論。

**何時有價值**：
- ✅ 原理論的實證證據薄弱（只有 1-2 篇研究）
- ✅ 跨文化驗證（如西方理論在亞洲情境）
- ✅ 跨時間驗證（如 1990 年代理論在 2020 年代）

**醫療 AI 範例**：

```text
有價值的複製研究：
「TAM 在 1989 年提出時，測試的是『文書處理軟體』採納。
我們在『醫療 AI』情境驗證 TAM，發現：
- 有用性 → 意圖：β = 0.28*（原理論 β = 0.6***）
- 易用性 → 意圖：β = 0.19ns（原理論 β = 0.4***）

理論貢獻：
發現 TAM 的『邊界條件』——在高風險、高責任情境，TAM 預測力下降。」

→ 有價值：指出理論的適用邊界
```

**Sutton & Staw 的警告**：
> 複製研究必須說明「為什麼這個情境的複製很重要」，
> 不能只是「因為沒人做過醫療 AI，所以我做」。

---

#### 例外 2：方法論貢獻研究

**定義**: 提出新的測量工具、新的分析方法。

**何時有價值**：
- ✅ 開發新的測量量表（如「AI 可解釋性量表」）
- ✅ 提出新的分析技術（如「機器學習 + 問卷調查」整合）
- ✅ 解決既有方法的侷限（如「共同方法變異」的新控制策略）

**醫療 AI 範例**：

```text
Straub (1989) 的「量表驗證指南」：
- 沒有提出新理論
- 但提供了「如何驗證測量工具」的方法論框架
- 貢獻：提升 MIS 研究的測量品質

醫療 AI 的方法論貢獻範例：
「我們開發『醫療 AI 可解釋性量表』，包含 3 個維度：
1. 透明度（能理解 AI 如何運作）
2. 可理解性（能解讀 AI 的輸出）
3. 可追溯性（能追蹤 AI 的決策路徑）

貢獻：提供經過驗證的測量工具，讓未來研究能一致測量『可解釋性』。」
```

---

#### 例外 3：現象發現研究 (Phenomenon Discovery)

**定義**: 發現新的、重要的、反直覺的現象。

**何時有價值**：
- ✅ 現象違反既有理論預測
- ✅ 現象具有重要實務影響
- ✅ 現象開啟新的研究方向

**醫療 AI 範例**：

```text
現象發現（有價值）：
「我們調查 500 位醫師，發現『AI 準確率悖論』：
當 AI 準確率從 85% 提升至 95% 時，醫師採納率反而下降（65% → 52%）。

深度訪談發現原因：
『AI 太準確，我反而不敢用，因為我怕自己判斷錯了，
但又無法理解 AI 為什麼這麼準，萬一出事我說不清楚。』

貢獻：
發現『準確率提升 → 採納率下降』的反直覺現象，
挑戰『準確率越高越好』的假設，開啟新的研究方向。」
```

**Sutton & Staw 的標準**：
> 現象發現研究必須在 Discussion 中「呼籲未來研究解釋這個現象」，
> 而非宣稱「這就是理論」。

---

## 🏥 醫療 AI 整合應用

### 應用 1：理論貢獻自我檢查清單

**問題**：醫療 AI 研究者常被審稿者要求「強化理論貢獻」，但不知道如何改進。

**Sutton & Staw 的解決方案**：提供「理論檢查清單」，讓研究者自我評估。

```markdown
# 醫療 AI 研究理論貢獻檢查清單

## ✅ 核心問題檢查
- [ ] 我的研究回答了明確的「WHY」問題（而非「WHAT」或「HOW MUCH」）
- [ ] 這個「WHY」問題在既有文獻中尚未被回答
- [ ] 回答這個問題對理論發展有重要意義

## ✅ 理論機制檢查
- [ ] 我提出了新的「因果機制」（而非只引用既有理論）
- [ ] 我解釋了「A 如何影響 B」的心理/社會過程
- [ ] 我的機制解釋「為什麼」這個關係存在

## ✅ 理論邊界檢查
- [ ] 我明確界定理論的「適用情境」
- [ ] 我說明了「何時」這個機制有效、「何時」無效
- [ ] 我識別了關鍵的「調節變數」

## ✅ 理論挑戰檢查
- [ ] 我的理論挑戰或擴展了既有理論（而非只是驗證）
- [ ] 我提出了「反直覺」或「超越常識」的預測
- [ ] 我排除了「替代解釋」

## ✅ 理論可測試性檢查
- [ ] 我的理論推導出可測試的假設
- [ ] 我的假設是「理論的邏輯結果」（而非「文獻說過」）
- [ ] 我的資料能區分「我的理論」vs.「競爭理論」

## ❌ Sutton & Staw 的「五個不是」檢查
- [ ] 我的「理論」不只是「文獻回顧」
- [ ] 我的「理論」不只是「資料描述」
- [ ] 我的「理論」不只是「變數列表」
- [ ] 我的「理論」不只是「研究模型圖」
- [ ] 我的「理論」不只是「假設羅列」

## 🎯 理論貢獻類型（至少符合一項）
- [ ] **機制發現**：提出新的因果機制
- [ ] **邊界界定**：界定既有理論的適用範圍
- [ ] **矛盾解釋**：解釋既有理論無法解釋的矛盾
- [ ] **整合創新**：提出新的理論整合邏輯
- [ ] **構念開發**：提出新的理論構念
```

---

### 應用 2：從「實證發現」到「理論建構」的 4 步驟法

**問題**：研究者常有「有趣的資料發現」，但不知如何轉化為「理論貢獻」。

**Sutton & Staw 的建議**：用「Why-How-When-What」框架。

```text
步驟 1：從資料發現開始（WHAT）
實證發現：
「我們調查 300 位醫師，發現可解釋性與採納率相關 r = 0.68***，
但有用性與採納率相關 r = 0.31*。」

步驟 2：詢問「為什麼」（WHY）
理論問題：
「為什麼可解釋性的影響 > 有用性的影響？
這與 TAM 的預測相反（TAM 預測有用性最重要）。」

步驟 3：提出機制解釋（HOW）
理論機制：
「醫療決策的法律責任機制：
醫師採納 AI 建議時，需承擔法律責任
→ 必須能向法院/家屬解釋決策依據
→ 黑箱 AI 無法解釋 → 法律風險 → 規避使用
→ 可解釋 AI 可說明依據 → 降低法律風險 → 採納」

步驟 4：界定邊界條件（WHEN）
理論邊界：
「這個機制在『高風險、高責任』情境有效：
✅ 適用：癌症診斷、手術決策、急診判斷
❌ 不適用：健康建議 App、排班系統、掛號系統」

步驟 5：推導可測試預測（WHAT IF）
理論預測：
「如果理論正確，應觀察到：
H1: 高風險決策，可解釋性影響更強（調節效果）
H2: 資深醫師（法律意識強），可解釋性影響更強
H3: 曾遭醫療訴訟者，可解釋性影響更強」
```

---

### 應用 3：理論貢獻的「電梯簡報」測試

**問題**：如何用 30 秒說清楚你的理論貢獻？

**Sutton & Staw 的建議**：用「3 句話總結理論」。

```text
句子 1：理論問題（WHY 問題）
「為什麼醫師更在意 AI 可解釋性，而非準確性？」

句子 2：理論答案（核心機制）
「因為醫療決策需要法律責任歸屬，黑箱 AI 無法說明決策依據，
導致醫師規避使用，即使它很準確。」

句子 3：理論貢獻（挑戰既有理論）
「這挑戰了 TAM 的『有用性主導』假設，指出在高責任情境，
『責任歸屬』比『效率提升』更重要。」
```

**醫療 AI 研究的電梯簡報範例**：

| 研究 | 3 句話理論簡報 |
|------|--------------|
| **研究 A** | ❌ 「我研究醫師採納 AI 的因素，發現有用性、易用性、信任都很重要。」（只是實證發現）|
| **研究 B** | ✅ 「為什麼醫師信任 AI 需要『先理解，後使用』，而非『先使用，後信任』？因為醫療不允許試錯，必須在採納前建立信任。這挑戰了『經驗建立信任』的傳統理論。」|

---

## 📚 學習檢查清單

**你已經掌握 Sutton & Staw (1995) 了嗎？請自我檢查：**

### 基礎概念（必須完全理解）
- [ ] 能區分「理論」vs.「實證發現」vs.「假設」
- [ ] 能說明「五個不是」：參考文獻、資料、變數、圖表、假設
- [ ] 能解釋「理論是什麼」：解釋 WHY 的因果邏輯故事
- [ ] 能判斷一篇論文「有無理論貢獻」

### 進階應用（能夠實作）
- [ ] 能從「實證發現」建構「理論機制」
- [ ] 能用「Why-How-When-What」框架發展理論
- [ ] 能區分「拼湊型整合」vs.「機制型整合」
- [ ] 能設計「競爭理論檢驗」來強化理論貢獻

### 專家級（能夠指導他人）
- [ ] 能診斷論文的「理論缺陷」（缺乏機制、缺乏邊界等）
- [ ] 能將「描述型個案研究」改寫為「理論建構型」
- [ ] 能評估「矛盾發現」是否構成理論貢獻
- [ ] 能撰寫「理論發展」章節（而非「文獻回顧」章節）

### 醫療 AI 特定技能
- [ ] 能識別醫療情境的「獨特機制」（法律責任、專業自主等）
- [ ] 能解釋「為什麼」醫療 AI 與一般 IT 採納不同
- [ ] 能從質性資料（訪談）歸納理論機制
- [ ] 能將「AI 可解釋性影響信任」深化為理論故事

---

## 🎓 寫給醫療 AI 研究者的最後提醒

Sutton & Staw (1995) 這篇論文的核心精神，用一句話總結：

> **「理論不是『知道什麼』(Knowing What)，而是『理解為什麼』(Understanding Why)。」**

在醫療 AI 研究中，這個提醒尤其重要：
- ❌ 「我們發現 AI 可解釋性影響採納」→ 這是實證發現，不是理論
- ✅ 「可解釋性影響採納，因為醫師需要法律責任歸屬」→ 這是理論

**理論的價值在於「可遷移性」(Transferability)**：
- 好的理論能解釋「為什麼」醫療 AI 採納與一般 IT 採納不同
- 好的理論能預測「何時」可解釋性重要、「何時」不重要
- 好的理論能指導「如何」設計 AI 系統（可解釋性 > 準確性）

**Sutton & Staw 的最後忠告**：
> 「如果你的論文被審稿者批評『缺乏理論貢獻』，
> 不要只是『增加文獻引用』或『增加變數』，
> 而是要回到核心問題：**我解釋了什麼『WHY』？**」

**理論建構是艱難的，但也是最有價值的**。
**它讓你的研究從『報告數據』提升為『創造知識』。**

---

*註：本頁面提供論文概要與 NotebookLM 4-card 學習方法，詳細內容請參閱原文。*
